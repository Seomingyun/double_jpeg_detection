{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Detect_manipulated photo_Website.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0SmWLVWXKnLx"},"source":["# 라이브러리 임포트"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6grfyEUuI0q","executionInfo":{"status":"ok","timestamp":1623663615038,"user_tz":-540,"elapsed":3509,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}},"outputId":"a40c190b-d2c4-4df0-b11e-a193b38ee6f7"},"source":["!pip install flask_ngrok\n","from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting flask_ngrok\n","  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0a3NqpAtjST","executionInfo":{"status":"ok","timestamp":1623663615039,"user_tz":-540,"elapsed":6,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["import os\n","\n","os.mkdir('templates')\n","os.mkdir('static')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KjgvdNjhLCYf","executionInfo":{"status":"ok","timestamp":1623663617893,"user_tz":-540,"elapsed":2858,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["import numpy as np\n","import math\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import os\n","import glob\n","from PIL import Image\n","from PIL import JpegImagePlugin\n","import argparse\n","import matplotlib.pyplot as plt\n","import easydict"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CTpiPcCRKwIM"},"source":["# model pth 다운로드"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"CFIQvP4Zuz2m","executionInfo":{"status":"ok","timestamp":1623663618818,"user_tz":-540,"elapsed":933,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}},"outputId":"bddd1a4e-7f01-4003-c52c-28a4c13908ce"},"source":["\n","import gdown\n","\n","url = 'https://drive.google.com/u/1/uc?id=1lQ_L-DpNjRFPZAOurTpS-0fCkqlNOLXI'\n","\n","annotation = 'model.zip'\n","gdown.download(url,annotation,quiet = False)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/u/1/uc?id=1lQ_L-DpNjRFPZAOurTpS-0fCkqlNOLXI\n","To: /content/model.zip\n","29.2MB [00:00, 56.3MB/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'model.zip'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Z7xReXGWvUsR","executionInfo":{"status":"ok","timestamp":1623663619696,"user_tz":-540,"elapsed":888,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["!unzip -qq 'model.zip'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RC7_3ebkK9Wc"},"source":["# Dataset 전처리"]},{"cell_type":"code","metadata":{"id":"7TNy2XhfIcOy","executionInfo":{"status":"ok","timestamp":1623663619696,"user_tz":-540,"elapsed":8,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["def read_q_table(file_name):\n","    jpg = JpegImagePlugin.JpegImageFile(file_name)\n","    qtable = JpegImagePlugin.convert_dict_qtables(jpg.quantization)\n","    Y_qtable = qtable[0]\n","    Y_qtable_2d = np.zeros((8, 8)) \n","\n","    qtable_idx = 0\n","    for i in range(0, 8):\n","        for j in range(0, 8):\n","            Y_qtable_2d[i, j] = Y_qtable[qtable_idx]\n","            qtable_idx = qtable_idx + 1\n","\n","    return Y_qtable_2d\n","\n","#caclulate DCT basis\n","def cal_scale(p,q):\n","\tif p==0:\n","\t\tap = 1/(math.sqrt(8))\n","\telse:\n","\t\tap = math.sqrt(0.25)\n","\tif q==0:\n","\t\taq = 1/(math.sqrt(8))\n","\telse:\n","\t\taq = math.sqrt(0.25)\n","\n","\treturn ap,aq\n","\n","def cal_basis(p,q):\n","\tbasis = np.zeros((8,8))\n","\tap,aq = cal_scale(p,q)\n","\tfor m in range(0,8):\n","\t\tfor n in range(0,8):\n","\t\t\tbasis[m,n] = ap*aq*math.cos(math.pi*(2*m+1)*p/16)*math.cos(math.pi*(2*n+1)*q/16)\n","\n","\treturn basis\n","\n","def load_DCT_basis_64(): \n","\tbasis_64 = np.zeros((8,8,64))\n","\tidx = 0\n","\tfor i in range(8):\n","\t\tfor j in range(8):\n","\t\t\tbasis_64[:,:,idx] = cal_basis(i,j)\n","\t\t\tidx = idx + 1\n","\treturn basis_64\n","\n","def load_DCT_basis_torch():\n","    DCT_basis_64 = load_DCT_basis_64()\n","    np_basis = np.zeros((64, 1, 8, 8)) #outchannel, inchannel, height, width\n","    for i in range(64):\n","        np_basis[i,0,:,:] = DCT_basis_64[:,:,i]\n","\n","    torch_basis = torch.from_numpy(np_basis)\n","    return torch_basis\n","\n","def calculate_f1(gt_path, result_array):\n","  a, b = result_array.shape\n","\n","  gt_img = Image.open(gt_path).convert(\"L\")\n","  gt_img = gt_img.resize((b, a)) \n","  gt_arr = np.asarray(gt_img)\n","\n","  threshold = 160\n","  gt_thres = np.zeros((a, b)).astype('uint8')\n","  res_thres = np.zeros((a, b)).astype('uint8')\n","\n","  for i in range(0, a):\n","    for j in range (0, b):\n","      if gt_arr[i][j] > threshold:\n","        gt_thres[i][j] = 255\n","      if result_array[i][j] > threshold:\n","        res_thres[i][j] = 255\n","    \n","  # calculate f1 score, accuracy\n","  tp, fp, fn, tn = 0, 0, 0, 0  \n","\n","  for i in range(0, a):\n","    for j in range (0, b):\n","      if gt_thres[i][j] == 255 and res_thres[i][j] == 255:    #tp\n","        tp = tp+1\n","      elif gt_thres[i][j] == 255 and res_thres[i][j] == 0:    # fn\n","        fn = fn+1\n","      elif gt_thres[i][j] == 0 and res_thres[i][j] == 255:    # fp\n","        fp = fp+1\n","      elif gt_thres[i][j] == 0 and res_thres[i][j] == 0:  # tn\n","        tn = tn+1\n","  \n","  f1_score = (2*tp)/((2*tp)+fp+fn)\n","  accuracy = (tp+tn)/(tp+tn+fp+fn)\n","\n","  return f1_score, accuracy\n","\n","def _extract_patches(Y, patch_size, stride):\n","    patches=list()\n","    h, w = Y.shape[0:2] # height, width\n","    \n","    H = (h - patch_size) // stride\n","    W = (w - patch_size) // stride\n","\n","    # patch: one JPEG block\n","    # patches: whole JPEG blocks\n","    for i in range(0,H*stride, stride):\n","        for j in range(0,W*stride,stride):\n","            patch = Y[i:i+patch_size, j:j+patch_size]\n","            patches.append(patch)\n","            \n","    return patches, H, W\n","    \n","def localizing_double_JPEG(Y, qvectors, net):\n","    net.eval()\n","    result=0\n","    PATCH_SIZE = 256\n","\n","    default_stride = 32 \n","    default_batchsize = 32 \n","\n","    \n","    qvectors = torch.from_numpy(qvectors).float()\n","    qvectors = qvectors.to(device) \n","    qvectors = torch.unsqueeze(qvectors, axis=0)\n","\n","    patches, H, W = _extract_patches(Y, patch_size=PATCH_SIZE, stride=default_stride)\n","\n","    # 256 * 256 pathces DJPEG detection\n","    result = np.zeros((H, W))\n","\n","    num_batches = math.ceil(len(patches) / default_batchsize)\n","\n","    result_flatten = np.zeros((H*W))\n","    for i in range(num_batches):\n","\n","        # batch_Y: one batch of image patches of Y channel\n","        if i==(num_batches-1): #last batch\n","            batch_Y = patches[i*default_batchsize:]\n","        else:\n","            batch_Y = patches[i*default_batchsize:(i+1)*default_batchsize]\n","\n","        batch_size = len(batch_Y) \n","        batch_Y = np.array(batch_Y) \n","\n","        batch_Y = torch.unsqueeze(torch.from_numpy(batch_Y).float().to(device), axis=1)\n","        \n","        batch_qvectors = torch.repeat_interleave(qvectors, batch_size, dim=0)\n","\n","        batch_output = net(batch_Y, batch_qvectors)\n","        batch_output = F.softmax(batch_output, dim=1)\n","\n","        result_flatten[(i*default_batchsize):(i*default_batchsize)+batch_size] = \\\n","                batch_output.detach().cpu().numpy()[:,0]\n","\n","    result = np.reshape(result_flatten, (H, W))\n","\n","    return result\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRk4kneBLK3a"},"source":["# Model 1~4"]},{"cell_type":"code","metadata":{"id":"zGNUBj--LO74","executionInfo":{"status":"ok","timestamp":1623663619697,"user_tz":-540,"elapsed":7,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["class Djpegnet1(nn.Module): \n","    def __init__(self, input_size, hidden_size, output_size, device, n_layers=1):\n","        super(Djpegnet1, self).__init__()\n","        self.dct_basis = load_DCT_basis_torch().float()\n","        self.dct_basis = self.dct_basis.to(device)\n"," \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n"," \n","        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=7)\n","        self.bn1 = nn.BatchNorm1d(num_features = hidden_size)\n","        self.pool1 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.conv2 = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=7)\n","        self.bn2 = nn.BatchNorm1d(num_features = hidden_size)\n","        self.pool2 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, dropout=0.2)\n","        \n","        self.fc1 = nn.Linear(hidden_size, 128)\n","        self.fc2 = nn.Linear(128, 2)\n"," \n","    def forward(self, x, qvectors, hidden=None):\n","        # feature extraction\n","        with torch.no_grad(): \n","            gamma=1e+06 # 10^6\n","            x = F.conv2d(x, self.dct_basis, stride=8) \n","            for b in range(-80, 81): \n","                x_ = torch.sum(torch.sigmoid(gamma*(x-b)), axis=[2,3])/1024 \n","                x_ = torch.unsqueeze(x_, axis=1) \n","                if b==-80:\n","                    features = x_\n","                else:\n","                    features = torch.cat([features, x_], axis=1)\n","            features = features[:, 0:160, :] - features[:, 1:161, :]\n","            features = torch.squeeze(features, axis=1)\n","            features = features[:,:,1:64] # remove DC values\n","        \n","        output = torch.relu(self.bn1(self.conv1(features)))\n","        output = self.pool1(output)\n"," \n","        output = torch.relu(self.bn2(self.conv2(output)))\n","        output = self.pool2(output)\n"," \n","        output = output.transpose(1, 2)\n"," \n","        output, hidden = self.gru(output, hidden)\n"," \n","        output_flat = output[:, -1, :]\n","        output = torch.cat([qvectors, output_flat], axis=1)\n","        output = output_flat\n"," \n","        output = torch.relu(self.fc1(output))\n","        output = self.fc2(output)\n"," \n","        return output"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"QAN-fdI-LVjF","executionInfo":{"status":"ok","timestamp":1623663619697,"user_tz":-540,"elapsed":6,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["class Djpegnet2(nn.Module): \n","  \n","    def __init__(self, input_size, hidden_size, output_size, device, n_layers=1):\n","        super(Djpegnet2, self).__init__()\n","        self.dct_basis = load_DCT_basis_torch().float()\n","        self.dct_basis = self.dct_basis.to(device)\n"," \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n"," \n","        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=5)\n","        self.bn1 = nn.BatchNorm1d(num_features = hidden_size)\n","        self.pool1 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.conv2 = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=5)\n","        self.bn2 = nn.BatchNorm1d(num_features = hidden_size)\n","        self.pool2 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, dropout=0.35)\n","        \n","        self.fc1 = nn.Linear(hidden_size, 128)\n","        self.fc2 = nn.Linear(128, 2)\n"," \n","    def forward(self, x, qvectors, hidden = None):\n","        # feature extraction\n","        with torch.no_grad(): \n","            gamma=1e+06 # 10^6\n","            x = F.conv2d(x, self.dct_basis, stride=8) \n","            for b in range(-60, 61): \n","                x_ = torch.sum(torch.sigmoid(gamma*(x-b)), axis=[2,3])/1024 \n","                x_ = torch.unsqueeze(x_, axis=1) \n","                if b==-60:\n","                    features = x_\n","                else:\n","                    features = torch.cat([features, x_], axis=1)\n","            features = features[:, 0:120, :] - features[:, 1:121, :]\n","            features = torch.squeeze(features, axis=1)\n","            features = features[:,:,1:64] # remove DC values\n","        \n","        output = torch.relu(self.bn1(self.conv1(features)))\n","        output = self.pool1(output)\n"," \n","        output = torch.relu(self.bn2(self.conv2(output)))\n","        output = self.pool2(output)\n"," \n","        output = output.transpose(1, 2)\n"," \n","        output, hidden = self.gru(output, hidden)\n"," \n","        output_flat = output[:, -1, :]\n","        output = torch.cat([qvectors, output_flat], axis=1)\n","        output = output_flat \n"," \n","        output = torch.relu(self.fc1(output))\n","        output = self.fc2(output)\n"," \n","        return output\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8McOqV4LVMH","executionInfo":{"status":"ok","timestamp":1623663619698,"user_tz":-540,"elapsed":7,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["class Djpegnet3(nn.Module): \n","    def __init__(self, input_size, hidden_size, output_size, device, n_layers=1):\n","        super(Djpegnet3, self).__init__()\n","        self.dct_basis = load_DCT_basis_torch().float()\n","        self.dct_basis = self.dct_basis.to(device)\n"," \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n"," \n","        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=7)\n","        self.bn1 = nn.BatchNorm1d(hidden_size)\n","        self.pool1 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.conv2 = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3)\n","        self.bn2 = nn.BatchNorm1d(hidden_size)\n","        self.pool2 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, bidirectional=True, dropout=0.2)\n","        \n","        self.fc1 = nn.Linear(2*hidden_size, 128)\n","        self.fc2 = nn.Linear(128, 2)\n"," \n","    def forward(self, x, qvectors, hidden=None):\n","        # feature extraction\n","        with torch.no_grad(): \n","            gamma=1e+06 # 10^6\n","            x = F.conv2d(x, self.dct_basis, stride=8) \n","            for b in range(-60, 61): \n","                x_ = torch.sum(torch.sigmoid(gamma*(x-b)), axis=[2,3])/1024 \n","                x_ = torch.unsqueeze(x_, axis=1) \n","                if b==-60:\n","                    features = x_\n","                else:\n","                    features = torch.cat([features, x_], axis=1)\n","                    \n","            features = features[:, 0:120, :] - features[:, 1:121, :]\n","            features = torch.squeeze(features, axis=1)\n","            features = features[:, :, 1:64]\n","        \n","        output = torch.relu(self.conv1(features))\n","        output = torch.relu(self.bn1(output))\n","        output = self.pool1(output)\n"," \n","        output = torch.relu(self.conv2(output))\n","        output = torch.relu(self.bn2(output))\n","        output = self.pool2(output)\n"," \n","        output = output.transpose(1, 2)\n","  \n","        output, hidden = self.gru(output, hidden)\n"," \n","        output_flat = output[:, -1, :]\n","        output = torch.cat([qvectors, output_flat], axis=1)\n","        output = output_flat\n"," \n","        output = torch.relu(self.fc1(output))\n","        output = self.fc2(output)\n"," \n","        return output"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSb7xKEQLVJK","executionInfo":{"status":"ok","timestamp":1623663619698,"user_tz":-540,"elapsed":6,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["class Djpegnet4(nn.Module): \n","    def __init__(self, input_size, hidden_size, output_size, device, n_layers=1):\n","        super(Djpegnet4, self).__init__()\n","        self.dct_basis = load_DCT_basis_torch().float()\n","        self.dct_basis = self.dct_basis.to(device)\n"," \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n"," \n","        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=5)\n","        self.bn1 = nn.BatchNorm1d(hidden_size)\n","        self.pool1 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.conv2 = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=5)\n","        self.bn2 = nn.BatchNorm1d(hidden_size)\n","        self.pool2 = nn.AvgPool1d(kernel_size=2)\n"," \n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, bidirectional=True, dropout=0.3)\n","        \n","        self.fc1 = nn.Linear(2*hidden_size, 128)\n","        self.fc2 = nn.Linear(128, 2)\n"," \n","    def forward(self, x, qvectors, hidden=None):\n","        # feature extraction\n","        with torch.no_grad():\n","            gamma=1e+06 # 10^6\n","            x = F.conv2d(x, self.dct_basis, stride=8) \n","            for b in range(-60, 61): \n","                x_ = torch.sum(torch.sigmoid(gamma*(x-b)), axis=[2,3])/1024 \n","                x_ = torch.unsqueeze(x_, axis=1) \n","                if b==-60:\n","                    features = x_\n","                else:\n","                    features = torch.cat([features, x_], axis=1)\n","            features = features[:, 0:120, :] - features[:, 1:121, :]\n","            features = torch.squeeze(features, axis=1)\n","        \n","        output = torch.relu(self.conv1(features))\n","        output = torch.relu(self.bn1(output))\n","        output = self.pool1(output)\n"," \n","        output = torch.relu(self.conv2(output))\n","        output = torch.relu(self.bn2(output))\n","        output = self.pool2(output)\n"," \n","        output = output.transpose(1, 2)\n","  \n","        output, hidden = self.gru(output, hidden)\n","        \n","        output_flat = output[:, -1, :]\n","        output = torch.cat([qvectors, output_flat], axis=1)\n","        output = output_flat\n"," \n","        output = torch.relu(self.fc1(output))\n","        output = self.fc2(output)\n","\n"," \n","        return output"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shEH3byRL-6s"},"source":["# 모델 삽입"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BXEpIl5LVGu","executionInfo":{"status":"ok","timestamp":1623663626736,"user_tz":-540,"elapsed":7043,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}},"outputId":"ecdbc4d0-05dc-4174-9d64-55c3d87b6a37"},"source":["global net1, net2, net3, net4\n","global device\n","\n","model_dir_name = './model'\n","\n","#######################################\n","selected_model1 = 'net1'\n","selected_model2 = 'net2'\n","selected_model3 = 'net3'\n","selected_model4 = 'net4'\n","\n","model_pth1 = selected_model1 + '.pth'\n","model_pth2 = selected_model2 + '.pth'\n","model_pth3 = selected_model3 + '.pth'\n","model_pth4 = selected_model4 + '.pth'\n","#######################################\n","\n","model_dir_name1 = os.path.join(model_dir_name, model_pth1)\n","model_dir_name2 = os.path.join(model_dir_name, model_pth2)\n","model_dir_name3 = os.path.join(model_dir_name, model_pth3)\n","model_dir_name4 = os.path.join(model_dir_name, model_pth4)\n","\n","args = easydict.EasyDict({ \"input_size\": 120, \"hidden_size\": 256, \"output_size\": 2, \"seq_size\": 64, \"n_layers\": 2})\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    \n","#load pre-trained weights\n","net1 = Djpegnet1(160, args.hidden_size, args.output_size, device, args.n_layers)\n","net2 = Djpegnet2(120, args.hidden_size, args.output_size, device, args.n_layers)\n","net3 = Djpegnet3(args.input_size, args.hidden_size, args.output_size, device, args.n_layers)\n","net4 = Djpegnet4(args.input_size, args.hidden_size, args.output_size, device, args.n_layers)\n","    \n","net1.load_state_dict(torch.load(model_dir_name1))\n","net2.load_state_dict(torch.load(model_dir_name2))\n","net3.load_state_dict(torch.load(model_dir_name3))\n","net4.load_state_dict(torch.load(model_dir_name4))\n","\n","net1.to(device)\n","net2.to(device)\n","net3.to(device)\n","net4.to(device)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Djpegnet4(\n","  (conv1): Conv1d(120, 256, kernel_size=(5,), stride=(1,))\n","  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n","  (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n","  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n","  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n","  (fc1): Linear(in_features=512, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"VWIx37eJLyym"},"source":["# HTML"]},{"cell_type":"code","metadata":{"id":"bR8kRdgrtxP2","executionInfo":{"status":"ok","timestamp":1623663626737,"user_tz":-540,"elapsed":23,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["text = '''\n","<html>\n","\n","<head>\n","<title>Capstone Image Forensic Website</title>\n","<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" />        \n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\"></script>\n","</head>\n","\n","    <body>\n","    <p><h1 align=\"center\">Capstone Image Forensic Website</h1></p>\n","\n","<div class=\"container\">\n","<div class=\"row\">\n","    <h2>Select a file to upload</h2>\n","\n","     <form action = \"/fileUpload\" method = \"POST\" enctype = \"multipart/form-data\">\n","        <dl>\n","            <p>\n","                <input type=\"file\" name=\"file\" class=\"form-control\" autocomplete=\"off\" required>\n","            </p>\n","        </dl>\n","        <p>\n","            <input type=\"submit\" value=\"Submit\" class=\"btn btn-info\">\n","        </p>\n","    </form>\n","</div>\n","</div>\n","\n","        </form>\n","    </body>\n","</html>\n","'''\n","file = open(\"templates/upload.html\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiQls2yPWosZ","executionInfo":{"status":"ok","timestamp":1623663626737,"user_tz":-540,"elapsed":21,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["text = '''\n","<html>\n","\n","<head>\n","<title>Capstone Image Forensic Website</title>\n","<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" />        \n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\"></script>\n","</head>\n","\n","\n","    <body>\n","    <p><h1 align=\"center\">Capstone Image Forensic Website</h1></p>\n","    <div class=\"container\">\n","        <h2 align=\"center\"> {{ data }} File Analysis Results </h2>\n","        <br><br>\n","\n","  <div class=\"container; text-align:center\">\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","Input Image\n","</div>\n","\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","Our Result\n","</div>\n","  </div>\n","\n","<br>\n","\n","\n","  <div class=\"container; text-align:center\">\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","{% if True %}\n","\t<img src=\"{{ori_path}}\" alt=\"input_image\"  width=\"250\">\n","{% endif %}\n","</div>\n","\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","{% if True %}\n","\t<img src={{img_path}} alt=\"ground_truth\" width=\"250\">\n","{% endif %}\n","</div>\n","</div>\n","\n","\n","\n","</div>\n","<br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n","<div class=\"container\">\n","<div class=\"row\">\n","    <h2>Select a Ground Truth file to upload</h2>\n","\n","    <form action = \"/gtUpload\" method = \"POST\" enctype = \"multipart/form-data\">\n","        <dl>\n","            <p>\n","                <input type=\"file\" name=\"gtfile\" class=\"form-control\" autocomplete=\"off\" required>\n","            </p>\n","        </dl>\n","        <p>\n","            <input type=\"submit\" value=\"Submit\" class=\"btn btn-info\">\n","        </p>\n","    </form>\n","\n","</div>\n","</div>\n","\n","    </body>\n","</html>\n","'''\n","\n","\n","\n","\n","\n","\n","\n","file = open(\"templates/showResult.html\",\"w\")\n","file.write(text)\n","file.close()\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"IP4xqQaWPGpC","executionInfo":{"status":"ok","timestamp":1623663626738,"user_tz":-540,"elapsed":21,"user":{"displayName":"양유진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKHxUxI0bKA7TzZ0LdlQsmETvNxa99or-GPtt8=s64","userId":"16179677259730763273"}}},"source":["text = '''\n","<html>\n","\n","<head>\n","<title>Capstone Image Forensic Website</title>\n","<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" />        \n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\"></script>\n","</head>\n","\n","\n","    <body>\n","    <p><h1 align=\"center\">Capstone Image Forensic Website</h1></p>\n","    <div class=\"container\">\n","        <h2 align=\"center\">Our Result With Ground-Truth </h2>\n","        <br><br><br>\n","\n","\n","<div style=\" float: left; padding: 10px; width: 33%;text-align:center\">\n","Input Image\n","</div>\n","\n","<div style=\" float: left; padding: 10px; width: 33%;text-align:center\">\n","Our Result\n","</div>\n","\n","<div style=\"float: left; padding: 10px; width: 33%;text-align:center\">\n","Ground Truth\n","</div>\n","<br>\n","\n","\n","\n","\n","\n","<div style=\"float: left; padding: 10px; width: 33%;text-align:center\">\n","{% if True %}\n","\t<img src=\"{{ori_path}}\" alt=\"input_image\"  width=\"350\">\n","{% endif %}\n","</div>\n","\n","<div style=\" float: left; padding: 10px; width: 33%;text-align:center\">\n","{% if True %}\n","\t<img src={{img_path}} alt=\"ground_truth\" width=\"350\">\n","{% endif %}\n","</div>\n","\n","<div style=\" float: left; padding: 10px; width: 33%;text-align:center\">\n","{% if True %}\n","\t<img src={{gt_path}} alt=\"result_image\" width=\"350\">\n","{% endif %}\n","</div>\n","\n","\n","\n","</div>\n","\n","    </body>\n","</html>\n","'''\n","file = open(\"templates/showWithGT.html\",\"w\")\n","file.write(text)\n","file.close()\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xizyFdJkL1pz"},"source":["# FLASK"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"676Jv8jbtxNt","outputId":"c4cba410-52d9-4477-c200-0945d533c340"},"source":["from flask import Flask, render_template, request\n","from werkzeug.utils import secure_filename\n","app = Flask(__name__)\n","run_with_ngrok(app)   \n","\n","\n","@app.route('/fileUpload', methods = ['GET', 'POST'])\n","def upload_file():\n","    if request.method == 'POST':\n","        f = request.files['file']\n","        f.save('static/' + secure_filename(f.filename))\n","        print('파일 업로드 성공!(static/' ,secure_filename(f.filename) ,')')\n","        return make_result(secure_filename(f.filename))\n","\n","\n","# 업로드 HTML 렌더링\n","@app.route('/')\n","def render_file():\n","    return render_template('upload.html')\n","\n","\n","# GT 파일 업로드 처리\n","@app.route('/gtUpload', methods = ['GET', 'POST'])\n","def upload_gt():\n","    print(\"in upload_gt\")\n","    if request.method == 'POST':\n","        f = request.files['gtfile']\n","        f.save('static/' + secure_filename(f.filename))\n","        gt_dir_path = 'static/' + secure_filename(f.filename)\n","        print('GT 파일 업로드 성공!(' ,gt_dir_path , ')')\n","        return render_template('showWithGT.html', ori_path = file_dir_path, img_path = result_dir_path, gt_path = gt_dir_path)\n","\n","@app.route('/resultShow')\n","def make_result(filename):\n","  global file_dir_path\n","  global result_dir_path\n","  file_dir_path = 'static/' + filename\n","  result_dir_path = 'static/' + 'result_' + filename\n","\n","  im = Image.open(file_dir_path)\n","  im = im.convert('YCbCr')\n","  Y = np.array(im)[:,:,0] # Y channel  \n","  qvector = read_q_table(file_dir_path).flatten()\n","\n","  result1 = localizing_double_JPEG(Y, qvector, net1)\n","  print(\"result1: 완료\")\n","  result2 = localizing_double_JPEG(Y, qvector, net2)\n","  print(\"result2: 완료\")\n","  result3 = localizing_double_JPEG(Y, qvector, net3)\n","  print(\"result3: 완료\")\n","  result4 = localizing_double_JPEG(Y, qvector, net4)\n","  print(\"result4: 완료\")\n","\n","  result_mean = (result1 + result2 + result3 + result4) / 4\n","\n","  result = result_mean*255\n","\n","  \n","  a, b = result.shape\n","  threshold = 140\n","  for i in range(0, a):\n","    for j in range (0, b):\n","       if result[i][j] < threshold:\n","         result[i][j] = 0 \n","\n","  result = result.astype('uint8')  \n","  img_result = Image.fromarray(result)\n","  img_result.convert(\"L\")\n","\n","  img_result.save(result_dir_path)\n","  print(\"이미지 분석 완료\")\n","  return render_template('showResult.html', data = filename, ori_path = file_dir_path, img_path = result_dir_path)\n","\n","\n","\n","if __name__ == '__main__':\n","    # 서버 실행\n","    app.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"],"name":"stdout"},{"output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"],"name":"stderr"},{"output_type":"stream","text":[" * Running on http://b00ee52b3322.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [14/Jun/2021 09:40:45] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/Jun/2021 09:40:45] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [14/Jun/2021 09:40:47] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","[2021-06-14 09:41:01,139] ERROR in app: Exception on /fileUpload [GET]\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 2447, in wsgi_app\n","    response = self.full_dispatch_request()\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1953, in full_dispatch_request\n","    return self.finalize_request(rv)\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1968, in finalize_request\n","    response = self.make_response(rv)\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 2098, in make_response\n","    \"The view function did not return a valid response. The\"\n","TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.\n","127.0.0.1 - - [14/Jun/2021 09:41:01] \"\u001b[35m\u001b[1mGET /fileUpload HTTP/1.1\u001b[0m\" 500 -\n"],"name":"stderr"},{"output_type":"stream","text":["파일 업로드 성공!(static/ splicing.jpg )\n","result1: 완료\n","result2: 완료\n","result3: 완료\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [14/Jun/2021 09:41:33] \"\u001b[37mPOST /fileUpload HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["result4: 완료\n","이미지 분석 완료\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [14/Jun/2021 09:41:33] \"\u001b[37mGET /static/splicing.jpg HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/Jun/2021 09:41:38] \"\u001b[37mGET /static/result_splicing.jpg HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/Jun/2021 09:41:45] \"\u001b[37mPOST /gtUpload HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["in upload_gt\n","GT 파일 업로드 성공!( static/splicing_gt.jpg )\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [14/Jun/2021 09:41:45] \"\u001b[37mGET /static/splicing_gt.jpg HTTP/1.1\u001b[0m\" 200 -\n","[2021-06-14 09:41:47,052] ERROR in app: Exception on /gtUpload [GET]\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 2447, in wsgi_app\n","    response = self.full_dispatch_request()\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1953, in full_dispatch_request\n","    return self.finalize_request(rv)\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1968, in finalize_request\n","    response = self.make_response(rv)\n","  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 2098, in make_response\n","    \"The view function did not return a valid response. The\"\n","TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.\n","127.0.0.1 - - [14/Jun/2021 09:41:47] \"\u001b[35m\u001b[1mGET /gtUpload HTTP/1.1\u001b[0m\" 500 -\n"],"name":"stderr"},{"output_type":"stream","text":["in upload_gt\n"],"name":"stdout"}]}]}